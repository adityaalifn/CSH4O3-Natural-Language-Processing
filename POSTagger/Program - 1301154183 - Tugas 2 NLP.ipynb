{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas 2 Pemrosesan Bahasa Alami - POSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All Depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, TimeDistributed, Dense, Input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Load 1020 first line of dataset and split into 1000 data for training set and 20 data for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020 1020\n",
      "['kera', 'untuk', 'amankan', 'pesta olahraga'] ['NN', 'SC', 'VB', 'NN']\n",
      "['pemeringkat', 'efek', 'indonesia', 'pefindo', 'menegaskan', 'peringkat', 'ida-', 'untuk', 'pt', 'serasi', 'autoraya', 'sera', 'dan', 'outstanding', 'obligasi', 'i', 'tahun', '2003', 'sebesar', 'rp', '165', 'miliar', 'yang', 'akan', 'jatuh tempo', 'pada', 'juli', '2008', 'dengan', 'prospek', 'peringkat', 'tersebut', 'stabil', ',', 'kata', 'pefindo', 'dalam', 'pernyataan', '-nya', 'di', 'jakarta', ',', 'rabu', '.'] ['NNP', 'NNP', 'NNP', 'NNP', 'VB', 'NN', 'SYM', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'CC', 'FW', 'NNP', 'NNP', 'NN', 'CD', 'JJ', 'SYM', 'CD', 'CD', 'SC', 'MD', 'VB', 'IN', 'NNP', 'CD', 'SC', 'NN', 'NN', 'PR', 'JJ', 'Z', 'VB', 'NNP', 'IN', 'NN', 'PRP', 'IN', 'NNP', 'Z', 'NNP', 'Z']\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename, number_of_sentences=1020):\n",
    "    '''Fungsi untuk membaca dataset sejumlah kalimat yang diinginkan.'''\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    \n",
    "    with open(filename) as file:\n",
    "        contents = file.readlines()\n",
    "    \n",
    "    contents = [content.strip() for content in contents]\n",
    "    idx_line = 0\n",
    "    while idx_line < len(contents):\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        while not contents[idx_line].startswith('</kalimat'):\n",
    "            if not contents[idx_line].startswith('<kalimat'):\n",
    "                content = contents[idx_line].split('\\t')\n",
    "                sentence.append(content[0].lower())\n",
    "                tag.append(content[1])\n",
    "            idx_line += 1\n",
    "        sentences.append(sentence)\n",
    "        tags.append(tag)\n",
    "        idx_line += 2\n",
    "        if len(sentences) >= number_of_sentences:\n",
    "            break\n",
    "    return sentences, tags\n",
    "\n",
    "\n",
    "sentences, tags = load_dataset('dataset/dataset.tsv')\n",
    "sentences_train, tags_train, sentences_test, tags_test = sentences[:1000], tags[:1000], sentences[1000:1020], tags[1000:1020]\n",
    "\n",
    "print(len(tags), len(sentences))\n",
    "print(sentences_train[0], tags_train[0])\n",
    "print(sentences_test[0], tags_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSTagger Method\n",
    "Implement 3 POSTagger method:\n",
    "1. Baseline\n",
    "2. Statistic based (Feature Extraction and Neural Network)\n",
    "3. Sequence Model based (HMM-Viterbi)\n",
    "4. Long-Short Term Memory (Bonus - On Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Method\n",
    "Get the highest tag frequencies of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Method Accuracy:  85.44117647058823 %\n",
      "saya menyukai makan nasi\n",
      "['PRP', 'VB', 'VB', 'NN']\n"
     ]
    }
   ],
   "source": [
    "def get_word_tag_frequencies(sentences, tags):\n",
    "    '''Fungsi untuk mendapatkan semua kata dan jumlah tagnya pada keseluruhan dataset.'''\n",
    "    words_tag = {}\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        for word, tag_word in zip(sentence, tag):\n",
    "            if word in words_tag.keys():\n",
    "                if tag_word in words_tag[word].keys():\n",
    "                    words_tag[word][tag_word] += 1\n",
    "                else:\n",
    "                    words_tag[word][tag_word] = 1\n",
    "            else:\n",
    "                words_tag[word] = {tag_word: 1}\n",
    "    return words_tag\n",
    "\n",
    "def get_baseline_model(word_tag_frequencies):\n",
    "    '''Mendapatkan model baseline. Kata dengan tag terbanyak pada tiap kata.'''\n",
    "    baseline_model = {}\n",
    "    for word, tags in word_tag_frequencies.items():\n",
    "        baseline_model[word] = max(tags.items(), key=operator.itemgetter(1))[0] # Megambil tag dengan jumlah terbanyak pada suatu kata\n",
    "    baseline_model['OOV'] = 'NN' # Menambahkan token Out Of Vocabulary untuk kata yang tidak ada di vocabulary akan dijadikan NN\n",
    "    return baseline_model\n",
    "\n",
    "def get_accuracy(sentences, tags, baseline_model):\n",
    "    '''Mendapatkan akurasi dari model baseline'''\n",
    "    n_true = 0\n",
    "    n_word = 0\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        for word, tag_word in zip(sentence, tag):\n",
    "            n_word += 1\n",
    "            if word not in baseline_model:\n",
    "                word = 'OOV' # Kata menjadi OOV jika tidak ada di dalam baseline model\n",
    "            if baseline_model[word] == tag_word:\n",
    "                n_true += 1\n",
    "    return n_true / n_word * 100\n",
    "\n",
    "def post_tag_baseline_model(sentence, baseline_model):\n",
    "    tag = []\n",
    "    for word in sentence.split():\n",
    "        if word not in baseline_model:\n",
    "            tag.append('NN')\n",
    "        else:\n",
    "            tag.append(baseline_model[word])\n",
    "    return tag\n",
    "    \n",
    "    \n",
    "word_tag = get_word_tag_frequencies(sentences_train, tags_train)\n",
    "baseline_model = get_baseline_model(word_tag)\n",
    "# print(baseline_model)\n",
    "print('Baseline Method Accuracy: ', get_accuracy(sentences_test, tags_test, baseline_model), '%')\n",
    "print('saya menyukai makan nasi')\n",
    "print(post_tag_baseline_model('saya menyukai makan nasi', baseline_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.76796206\n",
      "Iteration 2, loss = 0.39087079\n",
      "Iteration 3, loss = 0.17088821\n",
      "Iteration 4, loss = 0.09164622\n",
      "Iteration 5, loss = 0.05681595\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tabul/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy:\n",
      "0.9264705882352942\n",
      "saya menyukai makan nasi\n",
      "['PRP' 'VB' 'VB' 'NN']\n"
     ]
    }
   ],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    }\n",
    "\n",
    "\n",
    " \n",
    "def transform_to_dataset(sentences, tags):\n",
    "    '''Mengubah kalimat di dataset menjadi siap untuk diolah'''\n",
    "    X, y = [], []\n",
    "    for sentence_idx in range(len(sentences)):\n",
    "        for index in range(len(sentences[sentence_idx])):\n",
    "            X.append(features(sentences[sentence_idx], index))\n",
    "            y.append(tags[sentence_idx][index])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_neural_network_classifier():\n",
    "    '''Mendapatkan model neural network.'''\n",
    "    return Pipeline([\n",
    "        ('vectorizer', DictVectorizer(sparse=False)),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(50, 25), learning_rate='adaptive', verbose=True, max_iter=5))\n",
    "    ])\n",
    "\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    '''Menampilkan prediksi POS tag pada suatu kalimat.'''\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return tags\n",
    "\n",
    "\n",
    "X, y = transform_to_dataset(sentences_train, tags_train)\n",
    "\n",
    "clf = get_neural_network_classifier()\n",
    "clf.fit(X, y)   \n",
    "\n",
    "print('Training completed')\n",
    "\n",
    "X_test, y_test = transform_to_dataset(sentences_test, tags_test) \n",
    "print(\"Neural Network Accuracy:\")\n",
    "print(clf.score(X_test, y_test))\n",
    " \n",
    "print('saya menyukai makan nasi')\n",
    "print(pos_tag(word_tokenize('saya menyukai makan nasi')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence based model - HMM with Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Algorithm Accuracy:\n",
      "63.382352941176464\n",
      "saya menyukai makan nasi\n",
      "['PRP', 'VB', 'NN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "def get_tag_count(tags):\n",
    "    '''Mendapatkan tag dan jumlahnya'''\n",
    "    tag_count = {}\n",
    "    for tag in tags:\n",
    "        for tag_word in tag:\n",
    "            if tag_word in tag_count:\n",
    "                tag_count[tag_word] += 1\n",
    "            else:\n",
    "                tag_count[tag_word] = 1\n",
    "    tag_count['<start>'] = len(tags)\n",
    "    return tag_count\n",
    "\n",
    "\n",
    "def get_tag_transition(tags):\n",
    "    '''Mendapatkan transisi dari tag ke tag lainnya dan jumlahnya.'''\n",
    "    tag_trans = {}\n",
    "    for tag in tags:\n",
    "        previous_word = '<start>'\n",
    "        for tag_word in tag:\n",
    "            if previous_word in tag_trans:\n",
    "                if tag_word in tag_trans[previous_word]:\n",
    "                    tag_trans[previous_word][tag_word] += 1\n",
    "                else:\n",
    "                    tag_trans[previous_word][tag_word] = 1\n",
    "            else:\n",
    "                tag_trans[previous_word] = {tag_word: 1}\n",
    "            previous_word = tag_word\n",
    "    return tag_trans\n",
    "\n",
    "\n",
    "def get_trans_prob_table(tag_trans, tag_count):\n",
    "    '''Mendapatkan table transisi dengan inputan transisi tag dan jumlah tag'''\n",
    "    trans_prob_table = {}\n",
    "    for tag1 in tag_count.keys():\n",
    "        trans_prob_table[tag1] = {}\n",
    "        for tag2 in tag_count.keys():\n",
    "            if tag2 in tag_trans[tag1].keys():\n",
    "                trans_prob_table[tag1][tag2] = tag_trans[tag1][tag2]/tag_count[tag1]\n",
    "            else:\n",
    "                continue\n",
    "    return trans_prob_table\n",
    "\n",
    "\n",
    "def get_emission_prob_table(word_tag, tag_count):\n",
    "    '''Mendapatkan tabel emisi dengan input tag dari word dan perhitungan jumlah tag.'''\n",
    "    emission_prob_table = copy.deepcopy(word_tag)\n",
    "    for word, tags in word_tag.items():\n",
    "        for tag in tags.keys():\n",
    "            emission_prob_table[word][tag] = word_tag[word][tag] / tag_count[tag]\n",
    "    return emission_prob_table\n",
    "    \n",
    "def viterbi_algorithm(sentence, transition_table, emission_table):\n",
    "    '''Memprediksi POS tag kalimat dari kalimat yang diberikan berdasarkan algoritma Viterbi'''\n",
    "    sentence_tag = []\n",
    "    viterbi_table  = {}\n",
    "    max_previous_value = 1\n",
    "    previous_tag = '<start>'\n",
    "    for word in sentence.split():\n",
    "        viterbi_table[word] = {}\n",
    "        if word not in emission_table:\n",
    "#             sentence_tag.append('NN')\n",
    "            viterbi_table[word]['NN'] = max_previous_value * transition_table[previous_tag]['NN']\n",
    "        else:\n",
    "            for tag in emission_table[word]:\n",
    "                if tag in transition_table[previous_tag]:\n",
    "                    viterbi_table[word][tag] = max_previous_value * transition_table[previous_tag][tag] * emission_table[word][tag]\n",
    "        previous_tag = max(viterbi_table[word].items(), key=operator.itemgetter(1))[0]\n",
    "        max_previous_value = viterbi_table[word][previous_tag]\n",
    "        sentence_tag.append(previous_tag)\n",
    "    return sentence_tag, viterbi_table\n",
    "\n",
    "def get_viterbi_accuracy(sentences, tags, viterbi_algorithm, transition_table, emission_table):\n",
    "    '''Mendapatkan akurasi dari model Viterbi'''\n",
    "    n_true = 0\n",
    "    n_word = 0\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        sentence = ' '.join(sentence)\n",
    "        pred_tag, _ = viterbi_algorithm(sentence, transition_table, emission_table)\n",
    "        for pred, truth in zip(pred_tag, tag):\n",
    "            n_word +=1\n",
    "            if pred == truth:\n",
    "                n_true += 1\n",
    "    return n_true / n_word * 100\n",
    "\n",
    "tag_count = get_tag_count(tags_train)\n",
    "word_tag = get_word_tag_frequencies(sentences_train, tags_train)\n",
    "tag_trans = get_tag_transition(tags_train)\n",
    "trans_prob_table = get_trans_prob_table(tag_trans, tag_count)\n",
    "emission_prob_table = get_emission_prob_table(word_tag, tag_count)\n",
    "print(\"Viterbi Algorithm Accuracy:\")\n",
    "print(get_viterbi_accuracy(sentences_test, tags_test, viterbi_algorithm, trans_prob_table, emission_prob_table))\n",
    "print('saya menyukai makan nasi')\n",
    "print(viterbi_algorithm('saya menyukai makan nasi', trans_prob_table, emission_prob_table)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Method using Long-Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data\n",
    "Preprocess data to fit for training using LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "tokenizer_sentences = Tokenizer()\n",
    "tokenizer_sentences.fit_on_texts(sentences)\n",
    "\n",
    "tokenizer_tags = Tokenizer()\n",
    "tokenizer_tags.fit_on_texts(tags)\n",
    "\n",
    "\n",
    "def preprocess_sentences(tokenizer, sentences, max_sequence_length=100):\n",
    "    sequence_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequence = pad_sequences(sequence_sentences, maxlen=max_sequence_length, padding='pre', truncating='pre')\n",
    "    return padded_sequence\n",
    "\n",
    "def tags_to_categorical(tokenizer, tags, max_sequence_length=100):\n",
    "    sequence_tags = tokenizer.texts_to_sequences(tags)\n",
    "    padded_tags = pad_sequences(sequence_tags, maxlen=max_sequence_length, padding='pre', truncating='pre')\n",
    "    return padded_tags\n",
    "\n",
    "def one_hot_tags(tags_categorical):\n",
    "    one_hot = []\n",
    "    for tags in tags_categorical:\n",
    "        one_hot_sentence = []\n",
    "        for tag in tags:\n",
    "            one_hot_tag = np.zeros(len(tokenizer_tags.word_counts)+1)\n",
    "            one_hot_tag[tag] = 1\n",
    "            one_hot_sentence.append(one_hot_tag)\n",
    "        one_hot.append(one_hot_sentence)\n",
    "    return one_hot\n",
    "\n",
    "sequence_train = preprocess_sentences(tokenizer_sentences, sentences_train)\n",
    "tags_categorical = tags_to_categorical(tokenizer_tags, tags_train)\n",
    "\n",
    "tags_categorical = one_hot_tags(tags_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 100 arrays: [array([[1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-424a38897668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m              metrics=['accuracy'])\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_categorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 100 arrays: [array([[1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n    ..."
     ]
    }
   ],
   "source": [
    "def get_LSTM_model(n_word, n_tag, max_sequence_length=100):\n",
    "    embedding_layer = Embedding(input_dim=n_word+1, output_dim=128)\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedding_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding_sequences)\n",
    "    predictions = TimeDistributed(Dense(n_tag + 1, activation='softmax'))(lstm)\n",
    "    \n",
    "    return Model(sequence_input, predictions)\n",
    "\n",
    "model = get_LSTM_model(len(tokenizer_sentences.word_counts), len(tokenizer_tags.word_counts))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(sequence_train, tags_categorical[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
