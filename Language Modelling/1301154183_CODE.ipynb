{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2WL2LkYPkp4"
   },
   "source": [
    "# Tugas NLP 1 - Bigram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTm66HJaPkp6"
   },
   "source": [
    "## Import All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "RHiZVy8wPkp6",
    "outputId": "6886afc4-a72e-42b2-a641-4bd5bac00aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tabul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjOHOvEIPkqA"
   },
   "source": [
    "## Read News File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1546
    },
    "colab_type": "code",
    "id": "lURGElN3PkqC",
    "outputId": "780d7774-9be2-46a8-db08-3c8297fb7fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Berita:  420\n"
     ]
    }
   ],
   "source": [
    "def read_csv(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    return df['title'].values, df['content'].values, df\n",
    "\n",
    "titles, contents, _ = read_csv('berita_nasional.csv')\n",
    "print('Jumlah Berita: ', len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2HesJUZPkqF"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RQi6Yc4PkqF"
   },
   "source": [
    "### Remove Unused Token and Adding Start and End Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJaENVOQPkqG",
    "outputId": "ac9590e9-ab02-4dd4-efea-1aa741340b17"
   },
   "outputs": [],
   "source": [
    "titles = ['sssss ' + title[:title.rfind('-')-1] + ' eeeee' for title in titles]\n",
    "contents = ['sssss ' + content[content.find('-')+1:] + ' eeeee' for content in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOTD2jEDPkqK"
   },
   "source": [
    "### Concatenating Titles and Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkAXvj9lPkqL"
   },
   "outputs": [],
   "source": [
    "corpus = [title + \" \" + content for title, content in zip(titles, contents)]\n",
    "corpus = ' '.join(corpus)\n",
    "corpus = corpus.lower()\n",
    "corpus = corpus.replace('.', ' . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00_yR1HgPkqN"
   },
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yODKpqidPkqO"
   },
   "outputs": [],
   "source": [
    "corpus = nltk.word_tokenize(corpus)\n",
    "bigrams = list(nltk.bigrams(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rud1SrJIPkqQ"
   },
   "source": [
    "### Term and Bigram Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kl7u-BSjPkqR",
    "outputId": "9cde2ee6-e0e6-4423-96f5-9a5d38ca0b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting frequencies ...\n"
     ]
    }
   ],
   "source": [
    "def count_frequencies(corpus):\n",
    "    frequencies = dict(zip(set(corpus), [corpus.count(word) for word in set(corpus)]))\n",
    "    return frequencies\n",
    "\n",
    "print('Counting frequencies ...')\n",
    "term_frequencies = count_frequencies(corpus)\n",
    "bigram_frequencies = count_frequencies(bigrams)\n",
    "\n",
    "print(term_frequencies)\n",
    "print(bigram_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9DDDoLzPkqZ"
   },
   "source": [
    "## Bigram Probability\n",
    "Using smoothing / add one if there is zero value in term frequencies (nominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyj3UPMvPkqa"
   },
   "outputs": [],
   "source": [
    "def count_probabilty(current_word, next_word, term_frequencies, bigram_frequencies):\n",
    "    try:\n",
    "        return (bigram_frequencies[(current_word, next_word)]) / (term_frequencies[current_word] + len(term_frequencies.keys()))\n",
    "    except KeyError:\n",
    "        try:\n",
    "            return 1 / (term_frequencies[current_word] + len(term_frequencies.keys()))\n",
    "        except KeyError:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsTvpu9dPkqc",
    "outputId": "cf14d66f-d82b-42f9-e9a2-57d95612b515"
   },
   "outputs": [],
   "source": [
    "count_probabilty('menjaga', 'stabilitas', term_frequencies, bigram_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Next Word with Highest Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByF3_i65Pkqg",
    "outputId": "602830ad-3c24-4332-f13f-9c9bd378f779"
   },
   "outputs": [],
   "source": [
    "def get_next_word(current_word, term_frequencies, bigram_frequencies):\n",
    "    prob_word = []\n",
    "    for word, count in term_frequencies.items():\n",
    "        try:\n",
    "            prob_word.append(count_probabilty(current_word, word, term_frequencies, bigram_frequencies))\n",
    "        except KeyError:\n",
    "            prob_word.append(0)\n",
    "    return list(term_frequencies.keys())[prob_word.index(max(prob_word))], max(prob_word)\n",
    "    \n",
    "\n",
    "print(get_next_word('sssss', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('joko', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('presiden', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('saya', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('akan', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('melakukan', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('hahaha', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('wkwkwk', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('i', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('gracias', term_frequencies, bigram_frequencies))\n",
    "print(get_next_word('hola', term_frequencies, bigram_frequencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Perplexity from Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nN9f2vP2Pkqj"
   },
   "outputs": [],
   "source": [
    "def count_perplexity(sentence, term_frequencies, bigram_frequencies):\n",
    "    perplexity = 0.0\n",
    "    sentence = sentence.split()\n",
    "    sentence.insert(0,'sssss')\n",
    "    sentence.append('eeeee')\n",
    "    for idx in range(len(sentence)-1):\n",
    "        try:\n",
    "            perplexity -= math.log(count_probabilty(sentence[idx], sentence[idx+1], term_frequencies, bigram_frequencies), 2)\n",
    "        except:\n",
    "            perplexity -= float('-inf')\n",
    "#     return perplexity ** (1/4) this line got perplexity value too small, then python round down to 0.0\n",
    "    return math.pow(2, perplexity / (len(sentence)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_perplexity('presiden joko widodo akan meresmikan bendungan baru', term_frequencies, bigram_frequencies))\n",
    "print(count_perplexity('saya suka makan nasi merah dan minum susu biru', term_frequencies, bigram_frequencies))\n",
    "print(count_perplexity('tim badminton indonesia berhasil meraih medali emas terbanyak pada asian games 2018', term_frequencies, bigram_frequencies))\n",
    "print(count_perplexity('rupiah melemah menjadi diatas 15 ribu', term_frequencies, bigram_frequencies))\n",
    "print(count_perplexity('saya bingung kalimat apalagi yang akan digunakan untuk evaluasi perplexity', term_frequencies, bigram_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bigram.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
